{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Usage: display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from images_evaluation\n",
      "images_valid\n",
      "images_valid\n",
      "images_valid\n",
      "images_valid\n",
      "images_valid\n",
      "images_valid\n",
      "images_valid\n",
      "images_valid\n",
      "images_valid\n",
      "images_valid\n",
      "images_valid\n",
      "images_test\n",
      "images_test\n",
      "images_test\n",
      "images_test\n",
      "images_test\n",
      "images_test\n",
      "images_test\n",
      "images_test\n",
      "images_test\n",
      "Reading from images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n",
      "images_background\n"
     ]
    }
   ],
   "source": [
    "data_path = './data'\n",
    "dataset = {}\n",
    "\n",
    "set_types = ['images_background', 'images_valid', 'images_test']\n",
    "alpha_chars = {}\n",
    "for set_type in set_types:\n",
    "    alpha_chars[set_type] = set()\n",
    "    dataset[set_type] = {}\n",
    "\n",
    "for set_type in os.listdir(data_path):    \n",
    "    print('Reading from {}'.format(set_type))\n",
    "    set_path = os.path.join(data_path, set_type)\n",
    "    for alpha_idx, alphabet in enumerate(os.listdir(set_path)):\n",
    "        if alpha_idx > 10 and set_type == 'images_evaluation':\n",
    "            new_set_type = 'images_test' # Test\n",
    "        elif set_type == 'images_evaluation':\n",
    "            new_set_type = 'images_valid' # Valid\n",
    "        else:\n",
    "            new_set_type = set_type # Train\n",
    "        print(new_set_type)\n",
    "        dataset[new_set_type][alphabet] = {}\n",
    "        alphabet_path = os.path.join(set_path, alphabet)\n",
    "        # print(alphabet)\n",
    "        for _, character in enumerate(os.listdir(alphabet_path)):\n",
    "            alpha_chars[new_set_type].update(['__'.join([alphabet, character])])\n",
    "            dataset[new_set_type][alphabet][character] = []\n",
    "            character_path = os.path.join(alphabet_path, character)\n",
    "            for k, sample in enumerate(os.listdir(character_path)):\n",
    "                img = Image.open(os.path.join(character_path, sample))\n",
    "                dataset[new_set_type][alphabet][character] += [np.asarray(img, np.uint8)]\n",
    "                img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpCAAAAAAc6VLmAAABJ0lEQVR4nO2ZaxKDMAiEodP7X5n+\naNW2EyIQjIMuB+BzeWyMstCkeMwCgQTSaaRnViImIqLOdiZr4mmkDiq9TyoqfyI0FCc67Mpo5szU\ntAK4pSu1erKxDiZ1I5nU6XqaR2woZfjKVg+k8iTPlH/GN+iUAU1NV9uP0OZyRFi0T35hcTfyCvOQ\nhP5MjT0sr6Y/B3WwAn2Sn+TmfoUmQiIrFZwI/RhSI+5GXmEjvudDXdHLQQIJpP2w+l8RTU3j0zQe\npKnxDEWqB9IFSbF3o8htw06K3WW2MFdvFHTviZDv8i225impfSJGPzlesU8g3YZkXKoxkmfHSlUP\nJJBAKkmyGV8tTVNIbzM3la+MpokkETIeiJl/w5eUbfQBJCXq9AkkkEACCaTzSS+phyXkPUx22gAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=105x105 at 0x7F6D3651E0B8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpCAAAAAAc6VLmAAAA70lEQVR4nO3a0Q6DIAwF0Muy//9l\n9qRLthbtXcsM3r5qOBYoIWDrmBSPWZAkSTeQnoF3GwCALfV4Tm2axFLMOHEUNSMoyp4RDTCGvrMI\ngHhO/f0lxdJGxS26csPWQHKa6gfPCemQypNcirJsaWvK6yCGcnLaKb5+TkpkDzHSHmlJ+VJ2UoOc\nkqlR7+VSK+5YJEmSJEmSJEmSri95Zyx5O/+hlM+YUgljSFXO14yog/xTt98OVE5LFcf1llRzLfC3\nyi285Vhx3ZMkSdKNpI7wGhm5f/qkYnH53pN0T4moGj+a/sSQJEmSJEmSJElaWnoBzDkc5V6PuxoA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=105x105 at 0x7F6D3651E860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpCAAAAAAc6VLmAAAA70lEQVR4nO3awQ4CIQwEUDD+/y/X\ng240EWRgGlLq9OTB5dkCDZKtVjbFbReUUrovP1mvD+BMO+RUx19xkkAq44qo5M59Vg4ZhM0J/6EZ\n5+kgyUrBdlSsvjfb4doxlRPY4RwkKjJKyIqAF/LPyFg9SZLCS/T2DZiTJEn/IhFbF5fYi4uw1ZOU\nXhqfYenD60DyGv8dner5Q4FXxHqynXmyjxGvhkdWtLciGv3UOCruPEmSJEmSJEmSJElKLi0fZJ1y\nAvyY1TMrjb8G6N3L3FsLzI1OzOpJkhRRGrej83LaIrXb4Vew7+bgcVT1JEmS9IoHJugX3WJlGiIA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=105x105 at 0x7F6D3651E1D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "(105, 105)\n"
     ]
    }
   ],
   "source": [
    "display(Image.fromarray(dataset['images_background']['Korean']['character06'][0]*255))\n",
    "display(Image.fromarray(dataset['images_background']['Korean']['character06'][5]*255))\n",
    "display(Image.fromarray(dataset['images_background']['Korean']['character06'][15]*255))\n",
    "\n",
    "print(len(dataset['images_background']['Korean']['character01']))\n",
    "print(dataset['images_background']['Korean']['character01'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 2, 11025) (30000,)\n",
      "(10000, 2, 11025) (10000,)\n",
      "(10000, 2, 11025) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def get_drawers(set_type, alpha, char, diff=False, alpha2=None, char2=None):\n",
    "    drawer1 = np.random.randint(len(dataset[set_type][alpha][char]))\n",
    "    drawer2 = drawer1\n",
    "    \n",
    "    if not diff:\n",
    "        while drawer2 == drawer1:\n",
    "            drawer2 = np.random.randint(len(dataset[set_type][alpha][char]))\n",
    "    else:\n",
    "        while drawer2 == drawer1:\n",
    "            drawer2 = np.random.randint(len(dataset[set_type][alpha2][char2]))\n",
    "    assert drawer1 != drawer2\n",
    "    \n",
    "    return drawer1, drawer2\n",
    "\n",
    "def get_random_dataset(set_type, set_size, shuffle_dataset=True):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for _ in range(set_size):\n",
    "        same_alpha, same_char = random.sample(alpha_chars[set_type], 1)[0].split('__')\n",
    "        drawer1, drawer2 = get_drawers(set_type, same_alpha, same_char)\n",
    "        inputs.append([dataset[set_type][same_alpha][same_char][drawer1].flatten(), \n",
    "                           dataset[set_type][same_alpha][same_char][drawer2].flatten()])\n",
    "        labels.append(1)\n",
    "\n",
    "        diff1_alpha, diff1_char = random.sample(alpha_chars[set_type], 1)[0].split('__')\n",
    "        diff2_alpha = diff1_alpha\n",
    "        diff2_char = diff1_char\n",
    "        while diff2_alpha == diff1_alpha and diff2_char == diff1_char:\n",
    "            diff2_alpha, diff2_char = random.sample(alpha_chars[set_type], 1)[0].split('__')\n",
    "        drawer1, drawer2 = get_drawers(set_type, diff1_alpha, diff1_char, True, diff2_alpha, diff2_char)\n",
    "        inputs.append([dataset[set_type][diff1_alpha][diff1_char][drawer1].flatten(), \n",
    "                           dataset[set_type][diff2_alpha][diff2_char][drawer2].flatten()])\n",
    "        labels.append(0)\n",
    "        \n",
    "    # Shuffle dataset\n",
    "    combined = list(zip(inputs, labels))\n",
    "    random.shuffle(combined)\n",
    "    \n",
    "    return zip(*combined)\n",
    "\n",
    "train_inputs, train_labels = get_random_dataset('images_background', 15000)\n",
    "valid_inputs, valid_labels = get_random_dataset('images_valid', 5000)\n",
    "test_inputs, test_labels = get_random_dataset('images_test', 5000)\n",
    "\n",
    "print(np.array(train_inputs).shape, np.array(train_labels).shape)\n",
    "print(np.array(valid_inputs).shape, np.array(valid_labels).shape)\n",
    "print(np.array(test_inputs).shape, np.array(test_labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpCAAAAAAc6VLmAAABgklEQVR4nO2Y247DMAhE7VX//5e9\nD23abuLMDJhWkXZ4jBIONwNxH+1L8vMtkEkmmWSSSSb9c9JNf7W31lp+GdB96mlGlLQqMmnVpetV\nxMOjleVQ82k5dCppAy3tuwqpBKTkqSBHGomAXikkptDoYVDvhzfTJMWMCTZNmrt00IxQjNQjIChS\nj4CgIWKJT+D7XanQQ4BJOogLJHFtgeMs1F4NSCBV1B0jUW2hVghIFUNJI92l7A7mOrtR3bVS0qdE\nDleiF/NX6+Xa0zUSkGAKAemuqexUcZ9E1NJ8UmecJpHNkgtKHSRtH5Z4hX0ahQEUu9FusTu+x21R\ne/lxa5zkBJ4wfeZCq4XwBvYIuijhpqFUOUVJBSP1vScKqlz9q/mrZIrSzkD0X+O0wAdr7eq90dgY\noT+Zd9HnEzaaD6semWfn5gtaQjP34/cRb6gTlmJCKHrbNxlQamPZK9aMDdyOpvS/5Dp7uUkmmWSS\nSSaZZJJJREZL7OiZbTknF4+eSSaZNJVfbetA3BglxsUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=105x105 at 0x7F6D3666C8D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpCAAAAAAc6VLmAAABFUlEQVR4nO3Y0Q6CMAyF4c34/q88\nLyAiuvWcjIYg+XujUeRr1zEmtZWT4nEWhISEhISEhHQ76Rl/XUsppbepqeurv+ERUpjB7o0k56Ta\n/SjGMvtUOwkclIZnjKwDNbXWGa4xNdOn5WytfHZmmyKjbmX1qVdfkvRzZmVlzr3zZnkcSEh3kMKl\n/GI1xbl+xXChSKxJJCSkxCcwVk3W8G13rQOSH8EYKGn5qVGUPMSsyZx+UVul1DxKp6Jrsiij5pwZ\nsULhNeFL47Tjvev7KOPadFej4zsWvZczIHP0DEqmY/ZJUjoXd0aIlJ1uT6zWw7+kYVzrnouEhISE\nhISElCXtn4u6MbMLm4trjx4SEhISEhISEtL/SC+qfSzMcbGu9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=105x105 at 0x7F6D35FFC470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sample_idx = np.random.randint(len(train_inputs))\n",
    "display(Image.fromarray(np.reshape(train_inputs[sample_idx][0]*255, (105, 105))))\n",
    "display(Image.fromarray(np.reshape(train_inputs[sample_idx][1]*255, (105, 105))))\n",
    "print(train_labels[sample_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=10, padding=0),\n",
    "            # nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=7, padding=0),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=4, padding=0),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, padding=0),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.ReLU())\n",
    "        self.fc = nn.Linear(9216, 4096)\n",
    "        self.distance_fc = nn.Linear(4096, 1)\n",
    "    \n",
    "    def siamese_net(self, inputs):\n",
    "        inputs = inputs.view(-1, 1, 105, 105)\n",
    "        out = self.layer1(inputs)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        fc = self.fc(out.view(out.size(0), -1))\n",
    "        return fc\n",
    "    \n",
    "    def distance_layer(self, input1, input2):\n",
    "        return F.sigmoid(self.distance_fc(torch.abs(input1 - input2))).squeeze(1)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        out1 = self.siamese_net(x1)\n",
    "        out2 = self.siamese_net(x2)\n",
    "        # print(out1.size())\n",
    "        # print(out2.size())\n",
    "        distance = self.distance_layer(out1, out2)\n",
    "        # print(distance.size())\n",
    "        \n",
    "        return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN() #.cuda()\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(cnn.parameters(), lr=1e-1, momentum=0.5)\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for train_idx in range(0, len(train_inputs), batch_size):\n",
    "        batch_inputs = train_inputs[train_idx:train_idx+batch_size]\n",
    "        batch_input1 = [b[0] for b in batch_inputs]\n",
    "        batch_input2 = [b[1] for b in batch_inputs]\n",
    "        batch_labels = train_labels[train_idx:train_idx+batch_size]\n",
    "        # print(np.array(batch_input1).shape)\n",
    "        # print(np.array(batch_input2).shape)\n",
    "        # print(np.array(batch_label).shape)\n",
    "        input1 = Variable(torch.Tensor(np.array(batch_input1).astype(np.float32)))# .cuda(0)\n",
    "        input2 = Variable(torch.Tensor(np.array(batch_input2).astype(np.float32)))# .cuda(0)\n",
    "        labels = Variable(torch.Tensor(np.array(batch_label).astype(np.float32)))# .cuda(0)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(input1, input2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (train_idx+1) % 1 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, train_idx+1, len(train_inputs), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
